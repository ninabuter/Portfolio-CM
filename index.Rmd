---
title: "Beyoncé's Discography"
author: "Nina Buter"
date: "Block 4"
output: 
  flexdashboard::flex_dashboard:
    storyboard: TRUE
    theme: yeti
---

```{r, echo = FALSE, include= FALSE}
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(plotly)
library(compmus)
library(flexdashboard)

```

```{r, echo = FALSE, include=FALSE}
DangerouslyInLove <- get_playlist_audio_features("", "3fMQZa4rswBIb9hs6n217m")
BDayDeluxeEdition <- get_playlist_audio_features("", "45P8KBiTPKwVWiFtLGUH1V")
IAmSachaFierce <- get_playlist_audio_features("", "6DOBEbYqJhld4nYXmv2d7Q")
Four <- get_playlist_audio_features("", "5gIFimP3dScVPvhg296hG7")
Beyoncé <- get_playlist_audio_features("", "2zCrPKo2K8KuP7PSCurVJd")
Lemonade <- get_playlist_audio_features("", "1JTaxwfuwmdYNdz2EgSJPz")
TheLionKing <- get_playlist_audio_features("", "2oqUwlwt0bY3nqkpFGrtaS")

```

```{r, echo = FALSE, include=FALSE}
Albums <-
  bind_rows(
    DangerouslyInLove %>% mutate(category = 'Dangerously In Love'),
    BDayDeluxeEdition %>% mutate(category = 'B-Day Deluxe Edition'),
    IAmSachaFierce %>% mutate(category = 'I Am Sacha Fierce'),
    Four %>% mutate(category = '4'),
    Beyoncé %>% mutate(category = 'Beyoncé'),
    Lemonade %>% mutate(category = 'Lemonade'),
    TheLionKing %>% mutate(category = 'The Lion King')
  )
Albums$category = factor(Albums$category, levels=c('Dangerously In Love', 'B-Day Deluxe Edition', 'I Am Sacha Fierce', '4', 'Beyoncé', 'Lemonade', 'The Lion King'))
```

### Structure of *Love On Top*. **Timbre** and **pitch** features shown in **self-similarity matrix**. 

```{r, echo=FALSE}
LoveOnTop <-
  get_tidy_audio_analysis("1z6WtY7X4HQJvzxC4UgkSf") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )
bind_rows(
  LoveOnTop %>% 
    compmus_self_similarity(pitches, "manhattan") %>% 
    mutate(d = d / max(d), type = "Chroma"),
  LoveOnTop %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  theme_classic() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  labs(x = "", y = "")
```

***
* In the two self-similarity matrixes (SSMs) you can see **chroma** as well as **timbre** features of the song *Love On Top*. Left, you see the chroma features, which show the **harmonic and melodic characteristics**. Right, the timbre features are shown, which show the changes in **timbre and instrumentation**. 
* In the chroma SSM, the dark diagonal lines from 0 to 180 seconds show **repetition in pitches**. The diagonal lines disappear after approximately 180 seconds. I think this happens because in the last two minutes of the song, the chorus is repeated and goes through **four key changes**, starting in C major and proceeding to Db major, then to D major, Eb major, and finally to E major. 
* In the timbre SSM, I can't really see any patterns. I think there's mostly an **similar timbre** throughout the song, and there are no significant changes in the beats or instrumentation. 

### **Chromagram** of *Love On Top*. Here, the **modulations** are shown more clearly. 

```{r, echo=FALSE}
LoveOnTop2 <-
  get_tidy_audio_analysis("1z6WtY7X4HQJvzxC4UgkSf") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

LoveOnTop2 %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

```

***
In this chromagram, the **key changes/modulations** of *Love On Top* (which I explained in the self-similarity matrix) are shown more clearly. As you can see, after 180 seconds the song switches from C major to Db major, D major, Eb major, and lastly to E major.


### Beyoncé's **discography**, and how has her music **changed** over time? 
Nowadays, Beyoncé is one of my favorite artists. One of the reasons I like to listen to Beyoncé's music is because she has such a diverse style in her albums. You can find various music genres come back in her music, like R&B, soul, (dance)pop, and hiphop. Therefore, I can mostly rely on at least some of her songs to be fitting to my mood. So far she has produced the following albums, among others:

* *Dangerously In Love*
* *B-Day*
* *I Am Sacha Fierce*
* *4*
* *Beyoncé*
* *Lemonade*
* *The Lion King*

On extravagant moments I like to listen to *4*, if I want to listen to old pop classics like Halo, I put on *I Am Sacha Fierce*, and when I'm not feeling well I listen to *Lemonade* for a boost of self-confidence. Because I like Beyoncé's diversity in styles and genres in and between albums, I will research with the SpotifyR features how her music has developed over the seven albums I've mentioned above. By applying and researching the SpotifyR audio features acousticness, energy, valence, dancaebility, speechness, and key of her albums, I'm hoping to see a shift in these features, so I know that I am not being illusional about the variety of Beyoncé's music. To dive a bit deeper in Beyoncé's songs, I would like to compare an earlier song *Halo* with a more recent song *Drunk In Love*, and see how the structure and timber of these songs differ on a deeper level. 


### Albums show a **high diversity in character**. A **shift** in **valence** and **acousticness** over time.

```{r, echo = FALSE}
VaEnAcDa <- ggplot(Albums, aes(x = valence, y = energy, color = acousticness, size = danceability, text=(paste("Track:", track.name, "<br>", 
            "Energy:", energy, "<br>",
            "Valence:", valence, "<br>",
            "Acousticness:", acousticness, "<br>",
            "Danceability:", danceability, "<br>")))) +
  geom_point(alpha = 0.5, size = 2) +
  geom_jitter(alpha = 0.5, width = 0.1) +
  facet_wrap(~category) +
  scale_color_gradient(low = "light blue", high = "black") +
  ggtitle('Beyoncé: Different Albums (In Chronological Order)') +
  xlab("Valence") + ylab("Energy") +
  theme(plot.title = element_text(family = 'Arial', 
                                  face = 'bold', 
                                  size = 15)) +
  theme(legend.title = element_text(family = 'Arial',
                                    face = 'bold',
                                    size = 10)) +
  theme(axis.title = element_text(family = 'Arial',
                                  size = 10)) +
  theme(axis.text.x = element_text(size = 6)) +
  theme(axis.text.y = element_text(size = 6))
  

ggplotly(VaEnAcDa, tooltip=c("text"))
```
***
* In these graphs, energy, valence, acousticness, and danceability are shown of Beyoncé's albums.
* Overall, the scatterplots show that the the songs of every album show a variety in values of energy, valence, acousticness and danceability. This shows that her albums are **not static**, but that the the albums have **a high diversity in character**. 
* Furthermore, the overall **valence level** of her albums has **shifted downwards** after the release of *Beyoncé*. Before *Beyoncé*, valence values of her songs were never even close to zero, whereas her last three albums contain more **lower-valued valence songs**. 
* Besides, acousticness has increased a bit in her last two albums, with values above 0.9. This was only seen once in her album *Dangerously In Love*, but not again untill *Lemonade* and *The Lion King*. 

### No clear changes in speechiness. 

```{r, echo = FALSE}
green <- "#1ed760"
yellow <- "#e7e247"
pink <- "#ed00d9"
blue <- "#17bebb"
orange <- "#eba834"
red <- "#ed0000"
purple <- "#8e00ed"
```

```{r, echo = FALSE}

Albums2 <- Albums%>%
  mutate(Speechiness=speechiness)


Speechness <- ggplot(Albums2, aes(x=reorder(track.name, Speechiness), y=Speechiness, fill=playlist_name, text=(paste("Track:", track.name, "<br>",
                                      "Speechiness:", speechiness))))+
  geom_col()+
  scale_fill_manual(values=c(green, yellow, pink, blue, orange, red, purple))+
  theme_minimal()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.major = element_blank(),
        legend.position="none")+
  ylab("Speechiness")+
  facet_wrap(~ category)+
  ggtitle("Speechiness of Albums (In Chronological Order)") +
    theme(plot.title = element_text(family = 'Arial', 
                                  face = 'bold', 
                                  size = 15)) +
    theme(axis.title = element_text(family = 'Arial'))

ggplotly(Speechness, tooltip=c("text"))

```
***
* In these graphs, the speechiness is shown of Beyoncé's albums. 
* Overall, the **speechiness** of Beyoncé's albums is more on the lower side, showing a lot of songs with **speechiness-values below 0.1**. According to Spotify API, this means that her albums contain mostly **non-speech-like songs** (songs < 0.33).  However, this is not what I personally experience when I listen to Beyoncé's music. Therefore, I'm not sure what to conclude from this graph.


### **Albums** show a **diversity in keys**, which means a **contrast between the songs** in her albums. 

```{r, echo = FALSE,include= FALSE}
KeyTable <- Albums%>%
  select(playlist_name, key)%>%
  group_by(playlist_name, key)%>%
  mutate(n=n())%>%
  unique()%>%
  group_by(key)%>%
  mutate(total=sum(n))%>%
  mutate(percent=round((n/total)*100))

head(KeyTable, 10)

```

```{r, echo = FALSE}
Toonsoort <- c('C', 'C#/Db', 'D', 'D#/Eb', 'E', 'F', 'F#/Gb', 'G', 'G#/Ab', 'A', 'A#/Bb', 'B')
KeyTable <- KeyTable%>%
  mutate(key2 = Toonsoort[key+1])
  
```

```{r, echo = FALSE}
KeyAlbums <- ggplot(KeyTable, aes(x=key2, fill=playlist_name, y = n, 
                                text = paste("Number of Songs: ", n, "<br>",
                                            "Percent Songs in Key: ", percent, "%")))+
  geom_bar(width=0.5, stat = "identity")+
  scale_x_discrete(limits = Toonsoort) +
  scale_fill_manual(values=c(green, yellow, pink, blue, orange, red, purple))+
  labs(x="Key", y="Percent of Songs")+
  guides(fill=guide_legend(title="Albums"))+
  theme_minimal()+
  ggtitle("Musical Key Percentage by Album") +
  theme(plot.title = element_text(family = 'Arial', 
                                  face = 'bold', 
                                  size = 15)) +
  theme(axis.title = element_text(family = 'Arial')) +
  theme(legend.title = element_text(family = 'Arial',
                                    face = 'bold',
                                    size = 10))

ggplotly(KeyAlbums, tooltip=c("text"))
```
***
* In this graph, the musical key percentage of Beyoncé's albums is shown. 
* After I created this graph, purely out of interest in the keys she uses for her songs, I was not sure if this graph would be of any use for my research. 
* However, the graph does show something I feel is important for my research, namely the **diversity in keys** she  uses in her albums. Besides the fact that the **D#/Eb key is neglected** in her songs, all the keys are used in songs of **at least three** of her albums. I'm not sure if this has in any way effect on my opinion that her albums are so diverse, but it shows some **contrast between the songs** in her albums.
